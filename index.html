<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <!-- Primary Meta Tags -->
  <meta name="title" content="Towards Motion-aware Referring Image Segmentation">
  <meta name="description" content="We propose MRaCL, a multimodal radial contrastive learning framework that improves motion understanding in Referring Image Segmentation.">
  <meta name="keywords" content="referring image segmentation, motion understanding, contrastive learning, multimodal, RIS, MRaCL, computer vision">
  <meta name="author" content="Chaeyun Kim, Seunghoon Yi, Yejin Kim, Yohan Jo, Joonseok Lee">
  <meta name="robots" content="index, follow">
  <meta name="language" content="English">
  
  <!-- Open Graph / Facebook -->
  <meta property="og:type" content="article">
  <meta property="og:site_name" content="Seoul National University">
  <meta property="og:title" content="Towards Motion-aware Referring Image Segmentation">
  <meta property="og:description" content="We propose MRaCL, a multimodal radial contrastive learning framework that improves motion understanding in Referring Image Segmentation.">
  <meta property="og:image" content="static/images/fig1_performance_gap.png">
  <meta property="article:published_time" content="2025-01-01T00:00:00.000Z">
  <meta property="article:author" content="Chaeyun Kim">

  <!-- Twitter -->
  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:title" content="Towards Motion-aware Referring Image Segmentation">
  <meta name="twitter:description" content="We propose MRaCL, a multimodal radial contrastive learning framework that improves motion understanding in Referring Image Segmentation.">
  <meta name="twitter:image" content="static/images/fig1_performance_gap.png">

  <!-- Academic/Research Specific -->
  <meta name="citation_title" content="Towards Motion-aware Referring Image Segmentation">
  <meta name="citation_author" content="Kim, Chaeyun">
  <meta name="citation_author" content="Yi, Seunghoon">
  <meta name="citation_author" content="Kim, Yejin">
  <meta name="citation_author" content="Jo, Yohan">
  <meta name="citation_author" content="Lee, Joonseok">
  <meta name="citation_publication_date" content="2025">
  
  <meta name="theme-color" content="#2563eb">
  
  <!-- Preconnect for performance -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link rel="preconnect" href="https://cdn.jsdelivr.net">

  <title>MRaCL: Towards Motion-aware Referring Image Segmentation</title>
  
  <!-- Favicon: 필요 시 favicon.ico를 static/images/에 추가 -->
  
  <!-- CSS -->
  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/index.css">
  <link rel="preload" href="static/css/fontawesome.all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  
  <noscript>
    <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  </noscript>
  
  <!-- Fonts -->
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700;800&display=swap" rel="stylesheet">
  
  <!-- JS -->
  <script defer src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script defer src="static/js/bulma-carousel.min.js"></script>
  <script defer src="static/js/bulma-slider.min.js"></script>
  <script defer src="static/js/index.js"></script>
  
  <!-- Structured Data -->
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "ScholarlyArticle",
    "headline": "Towards Motion-aware Referring Image Segmentation",
    "description": "We propose MRaCL, a multimodal radial contrastive learning framework that improves motion understanding in Referring Image Segmentation.",
    "author": [
      {"@type": "Person", "name": "Chaeyun Kim", "affiliation": {"@type": "Organization", "name": "Seoul National University"}},
      {"@type": "Person", "name": "Seunghoon Yi", "affiliation": {"@type": "Organization", "name": "Seoul National University"}},
      {"@type": "Person", "name": "Yejin Kim", "affiliation": {"@type": "Organization", "name": "Seoul National University"}},
      {"@type": "Person", "name": "Yohan Jo", "affiliation": {"@type": "Organization", "name": "Seoul National University"}},
      {"@type": "Person", "name": "Joonseok Lee", "affiliation": {"@type": "Organization", "name": "Seoul National University"}}
    ],
    "datePublished": "2025",
    "keywords": ["referring image segmentation", "motion understanding", "contrastive learning", "multimodal"],
    "isAccessibleForFree": true
  }
  </script>
</head>
<body>

  <!-- Scroll to Top Button -->
  <button class="scroll-to-top" onclick="scrollToTop()" title="Scroll to top" aria-label="Scroll to top">
    <i class="fas fa-chevron-up"></i>
  </button>

  <main id="main-content">

  <!-- ===== HERO SECTION ===== -->
  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-widescreen">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Towards Motion-aware<br>Referring Image Segmentation</h1>
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a href="#" target="_blank">Chaeyun Kim</a><sup>1,2*</sup>,</span>
              <span class="author-block">
                <a href="#" target="_blank">Seunghoon Yi</a><sup>1*</sup>,</span>
              <span class="author-block">
                <a href="#" target="_blank">Yejin Kim</a><sup>1</sup>,</span>
              <span class="author-block">
                <a href="#" target="_blank">Yohan Jo</a><sup>1</sup>,</span>
              <span class="author-block">
                <a href="http://www.joonseok.net/home.html" target="_blank">Joonseok Lee</a><sup>1&dagger;</sup></span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block"><sup>1</sup>Seoul National University&nbsp;&nbsp;&nbsp;&nbsp;<sup>2</sup>AIM Intelligence</span>
              <span class="eql-cntrb"><small><br><sup>*</sup>Equal Contribution&nbsp;&nbsp;&nbsp;<sup>&dagger;</sup>Corresponding Author</small></span>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- Paper PDF -->
                <span class="link-block">
                  <a href="#" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                  </a>
                </span>

                <!-- GitHub Code -->
                <span class="link-block">
                  <a href="#" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
                </span>

                <!-- arXiv -->
                <span class="link-block">
                  <a href="#" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                  </a>
                </span>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>


  <!-- ===== ABSTRACT ===== -->
  <section class="section hero is-light">
    <div class="container is-max-widescreen">
      <div class="columns is-centered has-text-centered">
        <div class="column">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
              Referring Image Segmentation (RIS) requires identifying objects from images based on textual descriptions. 
              We observe that existing methods significantly underperform on motion-related queries compared to appearance-based ones.
              To address this, we first introduce an efficient data augmentation scheme that extracts motion-centric phrases from original captions, exposing models to more motion expressions without additional annotations.
              Second, since the same object can be described differently depending on the context, we propose <strong>Multimodal Radial Contrastive Learning (MRaCL)</strong>, performed on fused image-text embeddings rather than unimodal representations. 
              For comprehensive evaluation, we introduce a new test split focusing on motion-centric queries, and introduce a new benchmark called <strong>M-Bench</strong>, where objects are distinguished primarily by actions. 
              Extensive experiments show our method substantially improves performance on motion-centric queries across multiple RIS models, maintaining competitive results on appearance-based descriptions.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>


  <!-- ===== MOTIVATION: Performance Gap ===== -->
  <section class="section">
    <div class="container is-max-widescreen">
      <div class="columns is-centered has-text-centered">
        <div class="column">
          <h2 class="title is-3">Why do RIS models struggle with motion?</h2>
        </div>
      </div>
      <div class="columns is-centered">
        <div class="column">
          <div class="content has-text-justified">
            <p>
              Existing RIS models are typically trained on datasets where objects are distinguished by appearance attributes 
              (e.g., color, clothing, position). As a result, they significantly underperform when the referring expression 
              describes an object through its <strong>motion or action</strong>. 
              We measure this performance gap across multiple state-of-the-art models on the G-Ref UMD test set.
            </p>
          </div>
          <!-- Side-by-side: Figure 1 + Table 1 -->
          <div class="columns is-centered is-vcentered motivation-figures">
            <div class="column is-half has-text-centered">
              <img src="static/images/fig1_performance_gap.png" 
                   alt="Appearance-based vs. motion-centric queries in RIS" 
                   class="motivation-img" loading="lazy" />
            </div>
            <div class="column is-half has-text-centered">
              <img src="static/images/table1_performance_gap.png" 
                   alt="Performance gap between appearance-centric and motion-centric queries" 
                   class="motivation-img" loading="lazy" />
            </div>
          </div>
          <p class="image-caption has-text-centered">
            <strong>Left: Appearance-based vs. motion-centric queries in RIS.</strong> Existing methods handle the former well but struggle with the latter.
            <strong>Right: Performance gap</strong> between appearance-centric and motion-centric queries on G-Ref UMD test set. All models show a significant drop on motion-centric queries.
          </p>
        </div>
      </div>
    </div>
  </section>


  <!-- ===== METHOD ===== -->
  <section class="section hero is-light">
    <div class="container is-max-widescreen">
      <div class="columns is-centered has-text-centered">
        <div class="column">
          <h2 class="title is-3">Method</h2>
          <div class="content has-text-justified" style="margin: 0 auto;">
            <p>
              Our approach enhances RIS models' understanding of action-centric expressions through two complementary strategies: 
              (1) data augmentation with motion-centric verb phrase extraction, and 
              (2) Multimodal Radial Contrastive Learning (MRaCL) on fused image-text embeddings using angular distance.
            </p>
          </div>
        </div>
      </div>

      <!-- Architecture Figure -->
      <div class="columns is-centered" style="margin-top: 1.5rem;">
        <div class="column">
          <div class="has-text-centered">
            <img src="static/images/architecture.png" 
                 alt="MRaCL architecture overview" 
                 class="architecture-img" loading="lazy" />
            <p class="image-caption">
              <strong>Overview of MRaCL.</strong> 
              (a) Given an image-text pair, we extract motion-centric verb phrases from the original caption and fuse both representations through a projection layer.
              (b) We compute angular similarity scores within each mini-batch, filter potential false negatives, and apply our MRaCL loss with margin penalty.
            </p>
          </div>
        </div>
      </div>

      <!-- Method Cards -->
      <div class="columns is-centered" style="margin-top: 2rem;">
        <div class="column is-10">
          <div class="columns is-multiline">

            <!-- Card 1 -->
            <div class="column is-one-third">
              <div class="method-card">
                <div class="method-number-badge">1</div>
                <h4 class="title is-5">Verb Phrase Augmentation</h4>
                <p>
                  We extract motion-centric verb phrases from original captions using an LLM, and use them as <em>supplementary</em> training examples.
                  The model learns that both the original and verb-phrase descriptions refer to the same target, reinforcing attention to motion semantics.
                </p>
              </div>
            </div>

            <!-- Card 2 -->
            <div class="column is-one-third">
              <div class="method-card">
                <div class="method-number-badge">2</div>
                <h4 class="title is-5">Multimodal Contrastive Learning</h4>
                <p>
                  Different expressions can describe the same object only within a specific image context.
                  We perform contrastive learning on <strong>fused cross-modal embeddings</strong> rather than unimodal representations, with false negative elimination.
                </p>
              </div>
            </div>

            <!-- Card 3 -->
            <div class="column is-one-third">
              <div class="method-card">
                <div class="method-number-badge">3</div>
                <h4 class="title is-5">MRaCL Loss</h4>
                <p>
                  We use <strong>angular distance</strong> as our similarity metric to overcome similarity saturation and anisotropy.
                  A margin penalty enforces minimum angular separation, making embeddings more discriminative.
                </p>
              </div>
            </div>

          </div>
        </div>
      </div>
    </div>
  </section>


  <!-- ===== RESULTS: OVERALL COMPARISON ===== -->
  <section class="section">
    <div class="container is-max-widescreen">
      <div class="columns is-centered has-text-centered">
        <div class="column">
          <h2 class="title is-3">Overall Comparison</h2>
        </div>
      </div>
      <div class="columns is-centered">
        <div class="column">
          <div class="content has-text-justified">
            <p>
              MRaCL consistently improves performance across <strong>all datasets and all models</strong>. 
              The degree of improvement correlates with the ratio of motion-centric queries per dataset &mdash;
              on G-Ref and Ref-ZOM (rich in verb-centric phrases), our method achieves substantial gains, 
              while still providing consistent improvements on verb-sparse datasets like RefCOCO and RefCOCO+.
            </p>
          </div>
          <div class="has-text-centered">
            <img src="static/images/table2_overall.png" 
                 alt="Overall comparison of RIS models with and without MRaCL" 
                 class="result-table-img" loading="lazy" />
            <p class="image-caption">
              <strong>Table 2: Overall comparison of the RIS models.</strong> 
              Green cells indicate statistically significant improvement. Gray cells mean neutral. 
              We have no case of significant performance drop (red).
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>


  <!-- ===== QUALITATIVE RESULTS ===== -->
  <section class="section hero is-light">
    <div class="container is-max-widescreen">
      <div class="columns is-centered has-text-centered">
        <div class="column">
          <h2 class="title is-3">Qualitative Results</h2>
        </div>
      </div>
      <div class="columns is-centered">
        <div class="column">
          <div class="content has-text-justified">
            <p>
              We illustrate the impact of our method on motion understanding through qualitative examples from MBench.
              Our method enables the RIS model to correctly identify targets based on <strong>their actions</strong>, 
              even when multiple visually similar objects are present.
            </p>
          </div>
          <!-- Qualitative Slider -->
          <div class="simple-slider" id="qualitative-slider">
            <div class="simple-slider-track">
              <div class="simple-slider-slide active">
                <div class="has-text-centered">
                  <img src="static/images/qualitative.png" 
                       alt="Qualitative examples from MBench test set" 
                       class="qualitative-carousel-img" loading="lazy" />
                  <p class="image-caption">
                    <strong>Qualitative examples from MBench test set.</strong>
                    (a) CRIS correctly identifies "a person snowboarding and holding a selfie stick" only with MRaCL.
                    (b) Improved focus on the specific action "arms crossed".
                    (c) MRaCL correctly disambiguates "the lion strolling forward" from the stationary one.
                  </p>
                </div>
              </div>
              <div class="simple-slider-slide">
                <div class="has-text-centered">
                  <img src="static/images/qualitative_additional.png" 
                       alt="Additional qualitative examples" 
                       class="qualitative-carousel-img" loading="lazy" />
                  <p class="image-caption">
                    <strong>Additional qualitative examples.</strong>
                  </p>
                </div>
              </div>
            </div>
            <button class="simple-slider-prev" onclick="slideNav('qualitative-slider', -1)" aria-label="Previous">&#10094;</button>
            <button class="simple-slider-next" onclick="slideNav('qualitative-slider', 1)" aria-label="Next">&#10095;</button>
            <div class="simple-slider-dots" id="qualitative-slider-dots"></div>
          </div>
        </div>
      </div>
    </div>
  </section>


  <!-- ===== ANALYSIS ===== -->
  <section class="section">
    <div class="container is-max-widescreen">
      <div class="columns is-centered has-text-centered">
        <div class="column">
          <h2 class="title is-3">Analysis</h2>
        </div>
      </div>

      <!-- Analysis Slider -->
      <div class="simple-slider" id="analysis-slider">
        <div class="simple-slider-track">
          <!-- Slide 1: Ablation -->
          <div class="simple-slider-slide active">
            <div class="analysis-slide">
              <div class="has-text-centered">
                <img src="static/images/table4_ablation.png" 
                     alt="Ablation on proposed components" 
                     class="analysis-slide-img" loading="lazy" />
              </div>
              <p class="analysis-slide-title"><strong>Ablation Study</strong></p>
              <p class="analysis-slide-desc">
                Each component &mdash; motion-centric augmentation, MRaCL loss, and false negative filtering &mdash; contributes to the overall improvement. Applying all three achieves the best performance.
              </p>
            </div>
          </div>
          <!-- Slide 2: Contrastive Loss -->
          <div class="simple-slider-slide">
            <div class="analysis-slide">
              <div class="has-text-centered">
                <img src="static/images/table_contrastive.png" 
                     alt="Comparison with various contrastive objectives" 
                     class="analysis-slide-img" loading="lazy" />
              </div>
              <p class="analysis-slide-title"><strong>Contrastive Loss Comparison</strong></p>
              <p class="analysis-slide-desc">
                MRaCL substantially outperforms L2, SimCSE, and InfoNCE (MCC) alternatives by leveraging angular distance with margin penalties.
              </p>
            </div>
          </div>
          <!-- Slide 3: Anisotropy -->
          <div class="simple-slider-slide">
            <div class="analysis-slide">
              <div class="has-text-centered">
                <img src="static/images/angle_dist.png" 
                     alt="Distribution of pairwise angular distances" 
                     class="analysis-slide-img" loading="lazy" />
              </div>
              <p class="analysis-slide-title"><strong>Resolving Anisotropy</strong></p>
              <p class="analysis-slide-desc">
                The original model shows tightly clustered embeddings. With MRaCL, the model utilizes a <strong>significantly wider representation space</strong>, enabling more discriminative embeddings for fine-grained action semantics.
              </p>
            </div>
          </div>
        </div>
        <button class="simple-slider-prev" onclick="slideNav('analysis-slider', -1)" aria-label="Previous">&#10094;</button>
        <button class="simple-slider-next" onclick="slideNav('analysis-slider', 1)" aria-label="Next">&#10095;</button>
        <div class="simple-slider-dots" id="analysis-slider-dots"></div>
      </div>
    </div>
  </section>


  <!-- ===== BIBTEX ===== -->
  <section class="section" id="BibTeX">
    <div class="container is-max-widescreen content">
      <div class="bibtex-header">
        <h2 class="title">BibTeX</h2>
        <button class="copy-bibtex-btn" onclick="copyBibTeX()" title="Copy BibTeX to clipboard">
          <i class="fas fa-copy"></i>
          <span class="copy-text">Copy</span>
        </button>
      </div>
      <pre id="bibtex-code"><code>@article{kim2025mracl,
  title={Towards Motion-aware Referring Image Segmentation},
  author={Kim, Chaeyun and Yi, Seunghoon and Kim, Yejin and Jo, Yohan and Lee, Joonseok},
  year={2025}
}</code></pre>
    </div>
  </section>

  </main>

  <!-- Footer -->
  <footer class="footer">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">
            <p>
              This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page. 
              This website is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative Commons Attribution-ShareAlike 4.0 International License</a>.
            </p>
          </div>
        </div>
      </div>
    </div>
  </footer>

</body>
</html>
